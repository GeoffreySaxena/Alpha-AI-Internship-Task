{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 6: NLP Alpha AI.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiW4x5t8O5dJj5IisR4cEl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fea68e99e1f34a7a8f14579d1628803e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f2322926970a4c8bb9d8b271cd59f721",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52926fba62094312b25ac6ea6cf28d00",
              "IPY_MODEL_8142e53275f341f08bf81b42f946cab7"
            ]
          }
        },
        "f2322926970a4c8bb9d8b271cd59f721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52926fba62094312b25ac6ea6cf28d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e8d7aa099c0242e5a24ec1714d3bcda1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1313945868,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1313945868,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73d474eb43f44115a4dfbe71e46636a0"
          }
        },
        "8142e53275f341f08bf81b42f946cab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5fc148fd918d47e398f8681a4ba6335d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.31G/1.31G [01:29&lt;00:00, 14.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf6ffae95a67458f8e90193433534d67"
          }
        },
        "e8d7aa099c0242e5a24ec1714d3bcda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73d474eb43f44115a4dfbe71e46636a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fc148fd918d47e398f8681a4ba6335d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf6ffae95a67458f8e90193433534d67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeoffreySaxena/Alpha-AI-Internship-Task/blob/main/Task_6_NLP_Alpha_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmAFTFDiXEFd"
      },
      "source": [
        "State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.\n",
        "\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V69jMxJbXg_o"
      },
      "source": [
        "GitHub Source: https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u4s6YLtU_YC",
        "outputId": "92bd6c39-250b-4022-a1ab-4c30e947bf54"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 37.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2he5iEyCXS6b"
      },
      "source": [
        "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\n",
        "\n",
        "You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\n",
        "\n",
        "The framework is based on PyTorch and Transformers and offers a large collection of pre-trained models tuned for various tasks. Further, it is easy to fine-tune your own models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIYVCzBKXU1c"
      },
      "source": [
        "GitHub Source: https://github.com/UKPLab/sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRHDT_FNUWbn",
        "outputId": "76ddd376-6826-4eb0-88ce-e4059f0c0a77"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/87/49dc49e13ac107ce912c2f3f3fd92252c6d4221e88d1e6c16747044a11d8/sentence-transformers-1.1.0.tar.gz (78kB)\n",
            "\r\u001b[K     |████▏                           | 10kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 20kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 30kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 40kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 61kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.0-cp37-none-any.whl size=119615 sha256=cb778e7c53ee2b8ab7cbf1fef9197c522b5c0b6e333f09ba627f0b117b35f6fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/cb/21/1066bff3027215c760ca14a198f698bca8fccb92e33e2327eb\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-1.1.0 sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkxYdr8LVntt"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fea68e99e1f34a7a8f14579d1628803e",
            "f2322926970a4c8bb9d8b271cd59f721",
            "52926fba62094312b25ac6ea6cf28d00",
            "8142e53275f341f08bf81b42f946cab7",
            "e8d7aa099c0242e5a24ec1714d3bcda1",
            "73d474eb43f44115a4dfbe71e46636a0",
            "5fc148fd918d47e398f8681a4ba6335d",
            "bf6ffae95a67458f8e90193433534d67"
          ]
        },
        "id": "VBiVfNJrVrOV",
        "outputId": "3bdb3621-a902-4af1-f44d-a42f7c551b39"
      },
      "source": [
        "model = SentenceTransformer('stsb-roberta-large')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fea68e99e1f34a7a8f14579d1628803e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1313945868.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmJlLtNEXzQp"
      },
      "source": [
        "og_blog: An original article written by me, Pratik Geoffrey Saxena\n",
        "\n",
        "re_blog: My article reworded by https://spinbot.com/\n",
        "\n",
        "og_blog Source: https://www.geoffreysaxena.tech/blogs/compose-a-poem-using-nlp-and-bert.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KlYzct1VujK",
        "outputId": "87703c09-f8cb-437f-975b-3b50ec6b1d52"
      },
      "source": [
        "og_blog = '''Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with \n",
        "the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural \n",
        "language data. The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
        "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
        "Computers have come a long way, and so has machine learning. With state-of-the-art models such as BERT (Bidirectional Encoder Representations from Transformers), \n",
        "natural language processing has become easy! I created a deep neural network to write poems, my model uses BERT as it's base. \n",
        "Though I may have not generated a 'Robert Frost' level poem, I learnt a lot in the process. I utilized almost all the concepts I'd learned ranging from image \n",
        "augmentation, scaling, defining callbacks, etc. So let's start off by importing our libraries:'''\n",
        "\n",
        "re_blog = '''Characteristic language preparing (NLP) is a subfield of semantics, software engineering, and computerized reasoning worried about the cooperations \n",
        "among PCs and human language, specifically how to program PCs to measure and examine a lot of normal language information. The outcome is a PC prepared to do \n",
        "\"understanding\" the substance of reports, including the logical subtleties of the language inside them. The innovation can then precisely extricate data and bits\n",
        "of knowledge contained in the archives just as arrange and sort out the actual records. PCs have progressed significantly, thus has AI. With cutting edge models like BERT\n",
        "(Bidirectional Encoder Representations from Transformers), regular language handling has gotten simple! I made a profound neural organization to compose sonnets, \n",
        "my model uses BERT as it's base. In spite of the fact that I may have not created a 'Robert Frost' level sonnet, I took in a great deal simultaneously. \n",
        "I used practically every one of the ideas I'd took in going from picture increase, scaling, characterizing callbacks, and so forth. So we should get going by bringing in our libraries:'''\n",
        "\n",
        "# encoding the blogs to get their embeddings\n",
        "embedding1 = model.encode(og_blog, convert_to_tensor=True)\n",
        "embedding2 = model.encode(re_blog, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "print(\"Original Blog:\", og_blog)\n",
        "print(\"Reworded Blog:\", re_blog)\n",
        "print(\"Similarity score:\", cosine_scores.item())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Blog: Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with \n",
            "the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural \n",
            "language data. The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
            "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
            "Computers have come a long way, and so has machine learning. With state-of-the-art models such as BERT (Bidirectional Encoder Representations from Transformers), \n",
            "natural language processing has become easy! I created a deep neural network to write poems, my model uses BERT as it's base. \n",
            "Though I may have not generated a 'Robert Frost' level poem, I learnt a lot in the process. I utilized almost all the concepts I'd learned ranging from image \n",
            "augmentation, scaling, defining callbacks, etc. So let's start off by importing our libraries:\n",
            "Reworded Blog: Characteristic language preparing (NLP) is a subfield of semantics, software engineering, and computerized reasoning worried about the cooperations \n",
            "among PCs and human language, specifically how to program PCs to measure and examine a lot of normal language information. The outcome is a PC prepared to do \n",
            "\"understanding\" the substance of reports, including the logical subtleties of the language inside them. The innovation can then precisely extricate data and bits\n",
            "of knowledge contained in the archives just as arrange and sort out the actual records. PCs have progressed significantly, thus has AI. With cutting edge models like BERT\n",
            "(Bidirectional Encoder Representations from Transformers), regular language handling has gotten simple! I made a profound neural organization to compose sonnets, \n",
            "my model uses BERT as it's base. In spite of the fact that I may have not created a 'Robert Frost' level sonnet, I took in a great deal simultaneously. \n",
            "I used practically every one of the ideas I'd took in going from picture increase, scaling, characterizing callbacks, and so forth. So we should get going by bringing in our libraries:\n",
            "Similarity score: 0.7902344465255737\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}